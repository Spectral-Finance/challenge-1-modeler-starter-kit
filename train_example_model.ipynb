{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environemnt Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:58:11.444644Z",
     "start_time": "2023-11-03T09:58:10.053727Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, brier_score_loss\n",
    "from torchmetrics.classification import BinaryROC, BinaryRecall, BinaryF1Score, BinaryAUROC, BinaryPrecisionRecallCurve\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# for reproducible results\n",
    "random_seed = 42\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.determenistic=True\n",
    "import random\n",
    "random.seed(random_seed)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = random_seed\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(random_seed)\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\" #specifically required for reproducibility with CuBLABS and CUDA\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:58:23.113274Z",
     "start_time": "2023-11-03T09:58:22.169511Z"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "training_dataframe = duckdb.query((f\"\"\"\n",
    "select * from '{'0xB79CDBC5Cd94a807CC5cc761e3eF4A6B9baC8939_training_data.parquet'}'\n",
    "where max_risk_factor < 100\n",
    "\"\"\")).df().drop(columns=['withdraw_amount_sum_eth'], inplace=False)\n",
    "\n",
    "training_cols = list(training_dataframe.columns.drop(\n",
    "    ['borrow_timestamp', 'wallet_address', 'borrow_block_number', 'target']))\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_dataframe[training_cols].to_numpy(),\n",
    "                                                    training_dataframe['target'].to_numpy(), test_size=0.2, random_state=random_seed)\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data ready for PyTorch model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:58:23.708780Z",
     "start_time": "2023-11-03T09:58:23.704503Z"
    }
   },
   "outputs": [],
   "source": [
    "class StratifiedBatchSampler:\n",
    "    \"\"\"Stratified batch sampling\n",
    "    Provides equal representation of target classes in each batch\n",
    "    \"\"\"\n",
    "    def __init__(self, y, batch_size, shuffle=True):\n",
    "        if torch.is_tensor(y):\n",
    "            y = y.numpy()\n",
    "        assert len(y.shape) == 1, 'label array must be 1D'\n",
    "        n_batches = int(np.ceil(len(y) / batch_size))\n",
    "        self.skf = StratifiedKFold(n_splits=n_batches, shuffle=shuffle)\n",
    "        self.X = torch.randn(len(y), 1).numpy()\n",
    "        self.y = y\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = n_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.skf.random_state = 42\n",
    "        for train_idx, test_idx in self.skf.split(self.X, self.y):\n",
    "            yield test_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:58:23.914082Z",
     "start_time": "2023-11-03T09:58:23.888072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define custom DataLoaders\n",
    "# train data\n",
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_data = TrainData(torch.from_numpy(X_train_scaled).type(torch.float), torch.from_numpy(y_train).type(torch.float))\n",
    "\n",
    "# test data    \n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "test_data = TestData(torch.from_numpy(X_test_scaled).type(torch.float), torch.from_numpy(y_test).type(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:58:52.326032Z",
     "start_time": "2023-11-03T09:58:52.300999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x2cfb03050>, <torch.utils.data.dataloader.DataLoader object at 0x2885c25d0>)\n",
      "Length of train dataloader: 101 batches of 3438\n",
      "Length of test dataloader: 26 batches of 3438\n",
      "Using number of workers: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "NUM_WORKERS = 0 # use all available CPU cores\n",
    "BATCH_SIZE = int(X_train.shape[0]/100) # ~1% of the training data\n",
    "# initialize DataLoaders\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_sampler=StratifiedBatchSampler(torch.tensor(y_train), batch_size=BATCH_SIZE), \n",
    "                              worker_init_fn=seed_worker, \n",
    "                              generator=g,\n",
    "                              num_workers=NUM_WORKERS)\n",
    "test_dataloader = DataLoader(dataset=test_data, \n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False,\n",
    "                             worker_init_fn=seed_worker,\n",
    "                             generator=g,\n",
    "                             num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f'Dataloaders: {train_dataloader, test_dataloader}') \n",
    "print(f'Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}')\n",
    "print(f'Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}')\n",
    "print(f'Using number of workers: {NUM_WORKERS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:58:59.275248Z",
     "start_time": "2023-11-03T09:58:59.217003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3405, 73]), torch.Size([3405]))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out what's inside the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:00.285476Z",
     "start_time": "2023-11-03T09:59:00.253151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3438, 73]), torch.Size([3438]))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out what's inside the testing dataloader\n",
    "test_features_batch, test_labels_batch = next(iter(test_dataloader))\n",
    "test_features_batch.shape, test_labels_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:01.382184Z",
     "start_time": "2023-11-03T09:59:01.377357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:01.836761Z",
     "start_time": "2023-11-03T09:59:01.832418Z"
    }
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f'Train time on {device}: {total_time:.3f} seconds')\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:02.242146Z",
     "start_time": "2023-11-03T09:59:02.237921Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get FPR, TPR and thresholds for the AUC curve\n",
    "def auc_fn(y_true, y_logits):\n",
    "    metric = BinaryROC(thresholds=None).to(device)\n",
    "    fpr, tpr, thresholds = metric(y_logits, y_true.type(torch.int))\n",
    "    J = tpr.detach().cpu().numpy() - fpr.detach().cpu().numpy()\n",
    "    ix = np.argmax(J)\n",
    "    best_thresh = thresholds[ix].item()\n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:03.345715Z",
     "start_time": "2023-11-03T09:59:03.340779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate recall\n",
    "def recall_fn(y_true, y_pred, y_logits):\n",
    "    best_thresh = auc_fn(y_true, y_logits)\n",
    "    metric = BinaryRecall(threshold=best_thresh).to(device)\n",
    "    return (metric(y_pred, y_true) * 100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:03.705226Z",
     "start_time": "2023-11-03T09:59:03.700592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate F1-Score\n",
    "def f1_score_fn(y_true, y_pred, y_logits):\n",
    "    best_thresh = auc_fn(y_true, y_logits)\n",
    "    metric = BinaryF1Score(threshold=best_thresh).to(device)\n",
    "    return (metric(y_pred, y_true) * 100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:03.975915Z",
     "start_time": "2023-11-03T09:59:03.971015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate AUROC\n",
    "def auroc_fn(y_true, y_logits):\n",
    "    metric = BinaryAUROC(thresholds=None).to(device)\n",
    "    return (metric(y_logits, y_true) * 100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:04.279906Z",
     "start_time": "2023-11-03T09:59:04.274047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Brier Score\n",
    "def brier_fn(y_true, y_logits):\n",
    "    y_pred_probs = torch.sigmoid(y_logits)\n",
    "    y_true, y_pred_probs = y_true.detach().cpu().numpy(), y_pred_probs.detach().cpu().numpy()\n",
    "    return brier_score_loss(y_true, y_pred_probs) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:04.567577Z",
     "start_time": "2023-11-03T09:59:04.561536Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate AUC of the PR curve\n",
    "def auc_pr_fn(y_true, y_logits):\n",
    "    metric = BinaryPrecisionRecallCurve(thresholds=None).to(device)\n",
    "    precision, recall, thresholds = metric(y_logits, y_true.type(torch.int))\n",
    "    precision, recall = precision.detach().cpu().numpy(), recall.detach().cpu().numpy()\n",
    "    return auc(recall, precision) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:04.901556Z",
     "start_time": "2023-11-03T09:59:04.878490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the KS-Statistic\n",
    "def ks_fn(y_true, y_logits):\n",
    "    y_true = pd.Series(y_true.detach().cpu().numpy())\n",
    "    y_pred_probs = torch.sigmoid(y_logits)\n",
    "    y_pred_probs = pd.Series(y_pred_probs.detach().cpu().numpy())\n",
    "    y_true = pd.concat([y_true, y_pred_probs], axis=1)\n",
    "    y_true.columns = ['y_test_class_actual', 'y_hat_test_proba']\n",
    "    y_true.sort_values('y_hat_test_proba', inplace = True)\n",
    "    y_true.reset_index(drop=True, inplace=True)\n",
    "    y_true['Cumulative N Population'] = y_true.index + 1\n",
    "    y_true['Cumulative N Bad'] = y_true['y_test_class_actual'].cumsum()\n",
    "    y_true['Cumulative N Good'] = y_true['Cumulative N Population'] - y_true['Cumulative N Bad']\n",
    "    y_true['Cumulative Perc Population'] = y_true['Cumulative N Population'] / y_true.shape[0]\n",
    "    y_true['Cumulative Perc Bad'] = y_true['Cumulative N Bad'] / y_true['y_test_class_actual'].sum()\n",
    "    y_true['Cumulative Perc Good'] = y_true['Cumulative N Good'] / (y_true.shape[0] - y_true['y_test_class_actual'].sum())\n",
    "    KS = max(y_true['Cumulative Perc Good'] - y_true['Cumulative Perc Bad'])\n",
    "    \n",
    "    return KS * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:05.325483Z",
     "start_time": "2023-11-03T09:59:05.320677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the difference between the median predicted probabilities of the two classes\n",
    "def prob_diff_fn(y_true, y_logits):\n",
    "    y_true = pd.Series(y_true.detach().cpu().numpy())\n",
    "    y_pred_probs = torch.sigmoid(y_logits)\n",
    "    y_pred_probs = pd.Series(y_pred_probs.detach().cpu().numpy())\n",
    "    y_true = pd.concat([y_true, y_pred_probs], axis=1)\n",
    "    y_true.columns = ['y_test_class_actual', 'y_hat_test_proba']\n",
    "    prob_density_diff = y_true.loc[y_true['y_test_class_actual'] == 1, 'y_hat_test_proba'].median() - y_true.loc[y_true['y_test_class_actual'] == 0, 'y_hat_test_proba'].median()\n",
    "    return prob_density_diff * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:05.978601Z",
     "start_time": "2023-11-03T09:59:05.975451Z"
    }
   },
   "outputs": [],
   "source": [
    "# model training function\n",
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               epochs: int,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               auc_fn,\n",
    "               recall_fn,\n",
    "               f1_score_fn,\n",
    "               auroc_fn,\n",
    "               brier_fn,\n",
    "               auc_pr_fn,\n",
    "               ks_fn,\n",
    "               prob_diff_fn,\n",
    "               device: torch.device = device):\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    train_loss, train_rec, train_f1, train_auroc, train_brier, train_aucpr, train_ks, train_prob_diff = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_logits = model(X).squeeze() # logits\n",
    "        y_pred = torch.round(torch.sigmoid(y_logits)) # predicted labels\n",
    "\n",
    "        # 2. Calculate loss & other metrics\n",
    "        loss = loss_fn(y_logits, y)\n",
    "        train_loss += loss\n",
    "        train_rec += recall_fn(y_true=y, y_pred=y_pred, y_logits=y_logits)\n",
    "        train_f1 += f1_score_fn(y_true=y, y_pred=y_pred, y_logits=y_logits)\n",
    "        train_auroc += auroc_fn(y_true=y, y_logits=y_logits)\n",
    "        train_brier += brier_fn(y_true=y, y_logits=y_logits)\n",
    "        train_aucpr += auc_pr_fn(y_true=y, y_logits=y_logits)\n",
    "        train_ks += ks_fn(y_true=y, y_logits=y_logits)\n",
    "        train_prob_diff += prob_diff_fn(y_true=y, y_logits=y_logits)\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and other metrics per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_rec /= len(data_loader)\n",
    "    train_f1 /= len(data_loader)\n",
    "    train_auroc /= len(data_loader)\n",
    "    train_brier /= len(data_loader)\n",
    "    train_aucpr /= len(data_loader)\n",
    "    train_ks /= len(data_loader)\n",
    "    train_prob_diff /= len(data_loader)\n",
    "    \n",
    "    if epochs % 5 == 0:\n",
    "        print(f'Training metrics:\\nLoss: {train_loss:.5f} | Recall: {train_rec:.2f}% | F1-Score: {train_f1:.2f}% | AUROC: {train_auroc:.2f}% | Brier Score: {train_brier:.2f}% | AUC PR: {train_aucpr:.2f}% | KS-Statistic: {train_ks:.2f}% | Pred Prob Diff: {train_prob_diff:.2f}%')\n",
    "    return {'loss': train_loss.item(),\n",
    "            'recall': train_rec,\n",
    "            'f1': train_f1,\n",
    "            'auroc': train_auroc,\n",
    "            'brier': train_brier,\n",
    "            'aucpr': train_aucpr,\n",
    "            'ks': train_ks,\n",
    "            'prob_diff': train_prob_diff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:06.632247Z",
     "start_time": "2023-11-03T09:59:06.627841Z"
    }
   },
   "outputs": [],
   "source": [
    "# model testing function\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              epochs: int,\n",
    "              auc_fn,\n",
    "              recall_fn,\n",
    "              f1_score_fn,\n",
    "              auroc_fn,\n",
    "              brier_fn,\n",
    "              auc_pr_fn,\n",
    "              ks_fn,\n",
    "              prob_diff_fn,\n",
    "              device: torch.device = device):\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    test_loss, test_rec, test_f1, test_auroc, test_brier, test_aucpr, test_ks, test_prob_diff = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    model.eval() # put model in eval mode\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            # Send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            y_logits = model(X).squeeze() # logits\n",
    "            y_pred = torch.round(torch.sigmoid(y_logits)) # predicted labels\n",
    "            \n",
    "            # 2. Calculate loss and other metrics\n",
    "            test_loss += loss_fn(y_logits, y)\n",
    "            test_rec += recall_fn(y_true=y, y_pred=y_pred, y_logits=y_logits)\n",
    "            test_f1 += f1_score_fn(y_true=y, y_pred=y_pred, y_logits=y_logits)\n",
    "            test_auroc += auroc_fn(y_true=y, y_logits=y_logits)\n",
    "            test_brier += brier_fn(y_true=y, y_logits=y_logits)\n",
    "            test_aucpr += auc_pr_fn(y_true=y, y_logits=y_logits)\n",
    "            test_ks += ks_fn(y_true=y, y_logits=y_logits)\n",
    "            test_prob_diff += prob_diff_fn(y_true=y, y_logits=y_logits)\n",
    "        \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_rec /= len(data_loader)\n",
    "        test_f1 /= len(data_loader)\n",
    "        test_auroc /= len(data_loader)\n",
    "        test_brier /= len(data_loader)\n",
    "        test_aucpr /= len(data_loader)\n",
    "        test_ks /= len(data_loader)\n",
    "        test_prob_diff /= len(data_loader)\n",
    "        \n",
    "        if epochs % 5 == 0:\n",
    "            print(f'Testing metrics:\\nLoss: {test_loss:.5f} | Recall: {test_rec:.2f}% | F1-Score: {test_f1:.2f}% | AUROC: {test_auroc:.2f}% | Brier Score: {test_brier:.2f}% | AUC PR: {test_aucpr:.2f}% | KS-Statistic: {test_ks:.2f}% | Pred Prob Diff: {test_prob_diff:.2f}%')\n",
    "        return {'loss': test_loss.item(),\n",
    "                'recall': test_rec,\n",
    "                'f1': test_f1,\n",
    "                'auroc': test_auroc,\n",
    "                'brier': test_brier,\n",
    "                'aucpr': test_aucpr,\n",
    "                'ks': test_ks,\n",
    "                'prob_diff': test_prob_diff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:07.459680Z",
     "start_time": "2023-11-03T09:59:07.456108Z"
    }
   },
   "outputs": [],
   "source": [
    "# model evaluation/validation function\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               auc_fn,\n",
    "               recall_fn,\n",
    "               f1_score_fn,\n",
    "               auroc_fn,\n",
    "               brier_fn,\n",
    "               auc_pr_fn,\n",
    "               ks_fn,\n",
    "               prob_diff_fn,\n",
    "               device: torch.device = device):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        other model validation metrics\n",
    "        device (str, optional): Target device to compute on. Defaults to device.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    loss, rec, f1, auroc, brier, aucpr, ks, prob_diff = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Make predictions with the model\n",
    "            y_logits = model(X).squeeze() # logits\n",
    "            y_pred = torch.round(torch.sigmoid(y_logits)) # predicted labels\n",
    "            \n",
    "            # Accumulate the loss and other metrics values per batch\n",
    "            loss += loss_fn(y_logits, y)\n",
    "            rec += recall_fn(y_true=y, y_pred=y_pred, y_logits=y_logits)\n",
    "            f1 += f1_score_fn(y_true=y, y_pred=y_pred, y_logits=y_logits)\n",
    "            auroc += auroc_fn(y_true=y, y_logits=y_logits)\n",
    "            brier += brier_fn(y_true=y, y_logits=y_logits)\n",
    "            aucpr += auc_pr_fn(y_true=y, y_logits=y_logits)\n",
    "            ks += ks_fn(y_true=y, y_logits=y_logits)\n",
    "            prob_diff += prob_diff_fn(y_true=y, y_logits=y_logits)\n",
    "        \n",
    "        # Scale loss and other metrics to find the averages per batch\n",
    "        loss /= len(data_loader)\n",
    "        rec /= len(data_loader)\n",
    "        f1 /= len(data_loader)\n",
    "        auroc /= len(data_loader)\n",
    "        brier /= len(data_loader)\n",
    "        aucpr /= len(data_loader)\n",
    "        ks /= len(data_loader)\n",
    "        prob_diff /= len(data_loader)\n",
    "        \n",
    "    return {'model_name': model.__class__.__name__, # only works when model was created with a class and is not compiled\n",
    "            'model_loss': loss.item(),\n",
    "            'model_rec': rec,\n",
    "            'model_f1': f1,\n",
    "            'model_auroc': auroc,\n",
    "            'model_brier': brier,\n",
    "            'model_aucpr': aucpr,\n",
    "            'model_ks': ks,\n",
    "            'model_prob_diff': prob_diff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:08.137930Z",
     "start_time": "2023-11-03T09:59:08.131466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "def plot_loss_curves(epoch_count, train_loss_values, test_loss_values):\n",
    "    plt.plot(epoch_count, train_loss_values, label='Train loss')\n",
    "    plt.plot(epoch_count, test_loss_values, label='Test loss')\n",
    "    plt.title('Training and test loss curves')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:08.714516Z",
     "start_time": "2023-11-03T09:59:08.706553Z"
    }
   },
   "outputs": [],
   "source": [
    "class ValidationLossEarlyStopping:\n",
    "    def __init__(self, patience=1, min_delta=0.0):\n",
    "        # number of epochs to allow for no improvement before stopping the execution\n",
    "        self.patience = patience  \n",
    "        # the minimum change to be counted as improvement\n",
    "        self.min_delta = min_delta  \n",
    "        # count the number of times the validation accuracy not improving\n",
    "        self.counter = 0  \n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    # return True when encountering _patience_ times decrease in validation loss \n",
    "    def early_stop_check(self, validation_loss):\n",
    "        if ((validation_loss + self.min_delta) < self.min_validation_loss):\n",
    "            self.min_validation_loss = validation_loss\n",
    "            # reset the counter if validation loss decreased at least by min_delta\n",
    "            self.counter = 0  \n",
    "        elif ((validation_loss + self.min_delta) > self.min_validation_loss):\n",
    "            # increase the counter if validation loss is not decreased by the min_delta\n",
    "            self.counter += 1 \n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hidden layer with ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:10.295255Z",
     "start_time": "2023-11-03T09:59:10.263339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "PredictLiquidationsV1(\n  (linear_layer_stack): Sequential(\n    (0): Linear(in_features=73, out_features=82, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=82, out_features=82, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.4, inplace=False)\n    (6): Linear(in_features=82, out_features=1, bias=True)\n    (7): Sigmoid()\n  )\n)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define NN architecture\n",
    "class PredictLiquidationsV1(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units):\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.2),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.4),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features),\n",
    "            nn.Sigmoid()\n",
    "        )        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "    \n",
    "# instantiate the model\n",
    "model_1 = PredictLiquidationsV1(input_features=X_train.shape[1],\n",
    "                                output_features=1,\n",
    "                                hidden_units=82).to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:12.210760Z",
     "start_time": "2023-11-03T09:59:12.181258Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopper = ValidationLossEarlyStopping(patience=1, min_delta=0.0)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor((y_train==0).sum()/y_train.sum()).type(torch.float))\n",
    "optimizer = optim.Adam(params=model_1.parameters(),\n",
    "                       lr=0.001,\n",
    "                       weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:59:13.204023Z",
     "start_time": "2023-11-03T09:59:13.008947Z"
    }
   },
   "outputs": [],
   "source": [
    "compiled_model_1 = torch.compile(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-03T09:59:15.105164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/30 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a57f17158f8e4a069c2665d4e4b6b46e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Training metrics:\n",
      "Loss: 0.84507 | Recall: 100.00% | F1-Score: 55.65% | AUROC: 67.37% | Brier Score: 26.50% | AUC PR: 60.66% | KS-Statistic: 28.82% | Pred Prob Diff: 2.48%\n",
      "Testing metrics:\n",
      "Loss: 0.80287 | Recall: 100.00% | F1-Score: 56.38% | AUROC: 77.68% | Brier Score: 24.29% | AUC PR: 75.12% | KS-Statistic: 48.32% | Pred Prob Diff: 6.97%\n",
      "Epoch: 5\n",
      "---------\n",
      "Training metrics:\n",
      "Loss: 0.76819 | Recall: 100.00% | F1-Score: 55.67% | AUROC: 77.69% | Brier Score: 23.31% | AUC PR: 76.83% | KS-Statistic: 49.33% | Pred Prob Diff: 13.17%\n",
      "Testing metrics:\n",
      "Loss: 0.76564 | Recall: 100.00% | F1-Score: 56.40% | AUROC: 78.47% | Brier Score: 23.13% | AUC PR: 77.60% | KS-Statistic: 51.34% | Pred Prob Diff: 13.46%\n"
     ]
    }
   ],
   "source": [
    "# Measure time\n",
    "train_time_start = timer()\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# Create empty loss lists to track values\n",
    "model_1_train_loss_values = []\n",
    "model_1_test_loss_values = []\n",
    "model_1_epoch_count = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_metrics = train_step(data_loader=train_dataloader, \n",
    "                               model=compiled_model_1, \n",
    "                               epochs=epoch,\n",
    "                               loss_fn=loss_fn,\n",
    "                               optimizer=optimizer,\n",
    "                               auc_fn=auc_fn,\n",
    "                               recall_fn=recall_fn,\n",
    "                               f1_score_fn=f1_score_fn,\n",
    "                               auroc_fn=auroc_fn,\n",
    "                               brier_fn=brier_fn,\n",
    "                               auc_pr_fn=auc_pr_fn,\n",
    "                               ks_fn=ks_fn,\n",
    "                               prob_diff_fn=prob_diff_fn\n",
    "    )\n",
    "    test_metrics = test_step(data_loader=test_dataloader,\n",
    "                             model=compiled_model_1,\n",
    "                             epochs=epoch,\n",
    "                             loss_fn=loss_fn,\n",
    "                             auc_fn=auc_fn,\n",
    "                             recall_fn=recall_fn,\n",
    "                             f1_score_fn=f1_score_fn,\n",
    "                             auroc_fn=auroc_fn,\n",
    "                             brier_fn=brier_fn,\n",
    "                             auc_pr_fn=auc_pr_fn,\n",
    "                             ks_fn=ks_fn,\n",
    "                             prob_diff_fn=prob_diff_fn\n",
    "    )\n",
    "    model_1_epoch_count.append(epoch)\n",
    "    model_1_train_loss_values.append(train_metrics['loss'])\n",
    "    model_1_test_loss_values.append(test_metrics['loss'])\n",
    "    \n",
    "    if early_stopper.early_stop_check(test_metrics['loss']):\n",
    "        print(f\"Stopped early at epoch: {epoch}\\n---------\")\n",
    "        print(f\"Training metrics:\\nLoss: {train_metrics['loss']:.5f} | Recall: {train_metrics['recall']:.2f}% | F1-Score: {train_metrics['f1']:.2f}% | AUROC: {train_metrics['auroc']:.2f}% | Brier Score: {train_metrics['brier']:.2f}% | AUC PR: {train_metrics['aucpr']:.2f}% | KS-Statistic: {train_metrics['ks']:.2f}% | Pred Prob Diff: {train_metrics['prob_diff']:.2f}%\")\n",
    "        print(f\"Testing metrics:\\nLoss: {test_metrics['loss']:.5f} | Recall: {test_metrics['recall']:.2f}% | F1-Score: {test_metrics['f1']:.2f}% | AUROC: {test_metrics['auroc']:.2f}% | Brier Score: {test_metrics['brier']:.2f}% | AUC PR: {test_metrics['aucpr']:.2f}% | KS-Statistic: {test_metrics['ks']:.2f}% | Pred Prob Diff: {test_metrics['prob_diff']:.2f}%\")\n",
    "        break\n",
    "\n",
    "train_time_end = timer()\n",
    "total_train_time_model_1 = print_train_time(start=train_time_start,\n",
    "                                            end=train_time_end,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plot_loss_curves(model_1_epoch_count, model_1_train_loss_values, model_1_test_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Calculate model 1 results\n",
    "model_1_results = eval_model(model=compiled_model_1,\n",
    "                             data_loader=test_dataloader,\n",
    "                             loss_fn=loss_fn,\n",
    "                             auc_fn=auc_fn,\n",
    "                             recall_fn=recall_fn,\n",
    "                             f1_score_fn=f1_score_fn,\n",
    "                             auroc_fn=auroc_fn,\n",
    "                             brier_fn=brier_fn,\n",
    "                             auc_pr_fn=auc_pr_fn,\n",
    "                             ks_fn=ks_fn,\n",
    "                             prob_diff_fn=prob_diff_fn,\n",
    "                             device=device)\n",
    "model_1_results.update({'model_name': model_1.__class__.__name__})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ELU Activation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export to ONNX"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "# Export the model_30 (requires onnx to be installed in the env)\n",
    "torch.onnx.export(model_1,\n",
    "                  torch.randn((1,X_train.shape[1]), requires_grad = True).to(device),\n",
    "                  'models/onnx/model_1.onnx',\n",
    "                  input_names = ['input'],\n",
    "                  output_names = ['output'],\n",
    "                  dynamic_axes = {'input' : {0 : 'batch_size'}, \n",
    "                                  'output' : {0 : 'batch_size'}})"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "x = next(iter(test_features_batch)).numpy().tolist()\n",
    "\n",
    "data = dict(input_data = [x])\n",
    "json.dump(data, open('proofs/model_1_input.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T09:52:28.290982Z",
     "start_time": "2023-11-03T09:52:28.256974Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ezkltestenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
